{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TArW6NRJ-JzJ"
      },
      "source": [
        "# Speech Recognition using Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkUz7w9o-JzT"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Problem statement:\n",
        "\n",
        "The task of automatic speech recognition (ASR) involves accurately transcribing spoken words into written text. This is a complex problem, as it requires mapping a sequence of audio features to a corresponding sequence of characters, words, or subword tokens. In addition, ASR must account for variations in pronunciation, accents, and speaking styles, making it a challenging task for both humans and machines.\n",
        "\n",
        "\n",
        "For the screening test i have used the LJSpeech dataset from the\n",
        "[LibriVox](https://librivox.org/) project. It consists of short\n",
        "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
        "the model will be similar to the original Transformer (both encoder and decoder)\n",
        "as proposed in the paper, \"Attention is All You Need\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1XhquSp-JzU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zG8qkC0-JzX"
      },
      "source": [
        "## Define the Transformer Input Layer\n",
        "\n",
        "When processing past target tokens for the decoder, we compute the sum of\n",
        "position embeddings and token embeddings.\n",
        "\n",
        "When processing audio features, we apply convolutional layers to downsample\n",
        "them (via convolution stides) and process local relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gptILhDa-JzX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TokenEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    A Keras layer that combines token embeddings with positional embeddings.\n",
        "\n",
        "    Args:\n",
        "        num_vocab (int): The size of the vocabulary, i.e. the maximum integer index + 1.\n",
        "        maxlen (int): The maximum length of input sequences.\n",
        "        num_hid (int): The dimensionality of the embedding space.\n",
        "\n",
        "    Input shape:\n",
        "        2D tensor with shape `(batch_size, sequence_length)`.\n",
        "\n",
        "    Output shape:\n",
        "        3D tensor with shape `(batch_size, sequence_length, num_hid)`.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Compute token embeddings and positional embeddings, and add them together.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input tensor, with shape `(batch_size, sequence_length)`.\n",
        "\n",
        "        Returns:\n",
        "            The output tensor, with shape `(batch_size, sequence_length, num_hid)`.\n",
        "\n",
        "        \"\"\"\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    A Keras layer that processes speech features using convolutional neural networks.\n",
        "\n",
        "    Args:\n",
        "        num_hid (int): The number of filters in each convolutional layer.\n",
        "        maxlen (int): The maximum length of input sequences.\n",
        "\n",
        "    Input shape:\n",
        "        3D tensor with shape `(batch_size, num_frames, num_features)`.\n",
        "\n",
        "    Output shape:\n",
        "        3D tensor with shape `(batch_size, sequence_length, num_hid)`.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Apply three convolutional layers to the input tensor, and return the result.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input tensor, with shape `(batch_size, num_frames, num_features)`.\n",
        "\n",
        "        Returns:\n",
        "            The output tensor, with shape `(batch_size, sequence_length, num_hid)`.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc1LOVr4-JzY"
      },
      "source": [
        "## Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder consists of multiple identical layers, each of which contains a multi-head attention mechanism followed by a feed-forward neural network (FFN).\n",
        "\n",
        "The TransformerEncoder takes as input a tensor inputs and applies three main operations:\n",
        "\n",
        "Multi-head attention: The tensor is passed through a layers.MultiHeadAttention layer with num_heads heads and key_dim equal to embed_dim. The output is a tensor that contains information about how each position in the input sequence is related to all other positions.\n",
        "\n",
        "Feed-forward network: The output of the multi-head attention layer is passed through a feed-forward network (FFN) composed of two dense layers with ReLU activation. The output of the FFN is a tensor with the same shape as the input tensor.\n",
        "\n",
        "Residual connections and layer normalization: The output of the FFN is added to the input tensor (with an intermediate normalization step), and the resulting tensor is passed through another layer normalization step."
      ],
      "metadata": {
        "id": "EBvmotH0a2yt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-w7fZfB-JzZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    \"\"\"\n",
        "    A Transformer encoder layer that consists of a multi-head self-attention mechanism\n",
        "    and a feedforward neural network. Layer normalization and dropout are also applied \n",
        "    before and after each sub-layer.\n",
        "\n",
        "    Args:\n",
        "        embed_dim (int): Dimensionality of the input and output embeddings.\n",
        "        num_heads (int): Number of attention heads to use.\n",
        "        feed_forward_dim (int): Dimensionality of the feedforward layer.\n",
        "        rate (float): Dropout rate to apply.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of the same shape as the input tensor, representing the output of the \n",
        "        Transformer encoder layer.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the Transformer encoder layer.\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n",
        "            training (bool): Whether the layer is in training mode or not.\n",
        "\n",
        "        Returns:\n",
        "            A tensor of the same shape as the input tensor, representing the output of the \n",
        "            Transformer encoder layer.\n",
        "\n",
        "        \"\"\"\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIByQDN6-Jza"
      },
      "source": [
        "## Transformer Decoder Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer Decoder layer has several components, including multi-head self-attention, multi-head attention with an encoder output, feed-forward network, and layer normalization.\n",
        "\n",
        "The self-attention component allows the model to attend to different positions in the input sequence and the encoder output component allows the model to consider the context of the input sequence. The feed-forward network applies non-linear transformations to the output of the attention components.\n",
        "\n",
        "Layer normalization is applied before and after each component to improve training stability."
      ],
      "metadata": {
        "id": "vf9hcIzZb2aN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3p_V2s--Jzb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    \"\"\"\n",
        "    TransformerDecoder layer of the Transformer model architecture.\n",
        "\n",
        "    It consists of a self-attention mechanism and an encoder-decoder attention mechanism,\n",
        "    followed by a feedforward neural network (FFN) layer. The layer also applies\n",
        "    layer normalization and dropout regularization.\n",
        "\n",
        "    Args:\n",
        "        embed_dim (int): Dimensionality of the embedding space.\n",
        "        num_heads (int): Number of attention heads to use.\n",
        "        feed_forward_dim (int): Dimensionality of the FFN layer.\n",
        "        dropout_rate (float): Dropout rate to use for regularization.\n",
        "\n",
        "    Attributes:\n",
        "        layernorm1 (LayerNormalization): Layer normalization for the self-attention output.\n",
        "        layernorm2 (LayerNormalization): Layer normalization for the encoder-decoder attention output.\n",
        "        layernorm3 (LayerNormalization): Layer normalization for the FFN output.\n",
        "        self_att (MultiHeadAttention): Self-attention mechanism.\n",
        "        enc_att (MultiHeadAttention): Encoder-decoder attention mechanism.\n",
        "        self_dropout (Dropout): Dropout layer for the self-attention output.\n",
        "        enc_dropout (Dropout): Dropout layer for the encoder-decoder attention output.\n",
        "        ffn_dropout (Dropout): Dropout layer for the FFN output.\n",
        "        ffn (Sequential): FFN layer.\n",
        "\n",
        "    Methods:\n",
        "        causal_attention_mask(batch_size, n_dest, n_src, dtype): Creates a causal attention mask.\n",
        "        call(enc_out, target): Applies the TransformerDecoder layer to the input.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a new instance of the TransformerDecoder layer.\n",
        "\n",
        "        Args:\n",
        "            embed_dim (int): Dimensionality of the embedding space.\n",
        "            num_heads (int): Number of attention heads to use.\n",
        "            feed_forward_dim (int): Dimensionality of the FFN layer.\n",
        "            dropout_rate (float): Dropout rate to use for regularization.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Self-attention and encoder-attention\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.self_dropout = layers.Dropout(dropout_rate)\n",
        "        self.enc_dropout = layers.Dropout(dropout_rate)\n",
        "        self.ffn_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"\n",
        "        Creates a causal attention mask.\n",
        "\n",
        "        The mask prevents flow of information from future tokens to current token by\n",
        "        masking the upper half of the dot product matrix in self-attention.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Size of the input batch.\n",
        "            n_dest (int): Number of target tokens.\n",
        "            n_src (int): Number of source tokens.\n",
        "            dtype (dtype): Data type to use for the mask.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Causal attention mask tensor.\n",
        "\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the decoder layer.\n",
        "\n",
        "        Args:\n",
        "            enc_out (Tensor): Output of the encoder layer with shape (batch_size, seq_len, embed_dim).\n",
        "            target (Tensor): Input to the decoder layer with shape (batch_size, seq_len, embed_dim).\n",
        "\n",
        "        Returns:\n",
        "            Output of the decoder layer with shape (batch_size, seq_len, embed_dim).\n",
        "        \"\"\"\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN9T9B9D-Jzb"
      },
      "source": [
        "## Complete the Transformer model\n",
        "\n",
        "The model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
        "During training, we give the decoder the target character sequence shifted to the left\n",
        "as input. During inference, the decoder uses its own past predictions to predict the\n",
        "next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv7DbAq6-Jzc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(keras.Model):\n",
        "    \"\"\"\n",
        "    Transformer model for sequence-to-sequence tasks.\n",
        "\n",
        "    Args:\n",
        "        num_hid (int): Number of hidden units in each Transformer layer.\n",
        "        num_head (int): Number of attention heads in each Transformer layer.\n",
        "        num_feed_forward (int): Number of units in the feedforward network in each Transformer layer.\n",
        "        source_maxlen (int): Maximum length of input sequences.\n",
        "        target_maxlen (int): Maximum length of target sequences.\n",
        "        num_layers_enc (int): Number of Transformer encoder layers.\n",
        "        num_layers_dec (int): Number of Transformer decoder layers.\n",
        "        num_classes (int): Number of classes in the output vocabulary.\n",
        "\n",
        "    Attributes:\n",
        "        loss_metric (keras.metrics.Mean): Mean loss metric for tracking loss during training.\n",
        "        num_layers_enc (int): Number of Transformer encoder layers.\n",
        "        num_layers_dec (int): Number of Transformer decoder layers.\n",
        "        target_maxlen (int): Maximum length of target sequences.\n",
        "        num_classes (int): Number of classes in the output vocabulary.\n",
        "        enc_input (SpeechFeatureEmbedding): Speech feature embedding layer.\n",
        "        dec_input (TokenEmbedding): Token embedding layer for decoder inputs.\n",
        "        encoder (keras.Sequential): Transformer encoder.\n",
        "        classifier (keras.layers.Dense): Final dense layer for classification.\n",
        "\n",
        "    Methods:\n",
        "        decode(enc_out, target):\n",
        "            Decodes target sequences using the encoder output.\n",
        "        call(inputs):\n",
        "            Executes the forward pass of the Transformer model.\n",
        "        train_step(batch):\n",
        "            Processes one batch during training.\n",
        "        test_step(batch):\n",
        "            Processes one batch during testing.\n",
        "        generate(source, target_start_token_idx):\n",
        "            Performs inference over one batch of inputs using greedy decoding.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        \"\"\"\n",
        "        Decodes target sequences using the encoder output.\n",
        "\n",
        "        Args:\n",
        "            enc_out (tf.Tensor): Encoder output tensor.\n",
        "            target (tf.Tensor): Target sequences tensor.\n",
        "\n",
        "        Returns:\n",
        "            Decoded target sequences tensor.\n",
        "        \"\"\"\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Executes the forward pass of the Transformer model.\n",
        "\n",
        "        Args:\n",
        "            inputs (tuple): Tuple of input sequences.\n",
        "\n",
        "        Returns:\n",
        "            Model output tensor.\n",
        "        \"\"\"\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"\n",
        "        Returns the metrics that the model should track during training and testing.\n",
        "\n",
        "        Returns:\n",
        "            List of metrics.\n",
        "        \"\"\"\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"\n",
        "        Executes one training step on a batch of data.\n",
        "\n",
        "        Args:\n",
        "            batch (dict): Dictionary containing the batch data.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with the loss value.\n",
        "        \"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "      \"\"\"\n",
        "      Executes one evaluation step on a batch of data.\n",
        "\n",
        "      Args:\n",
        "          batch (dict): Dictionary containing the batch data.\n",
        "\n",
        "      Returns:\n",
        "          Dictionary with the loss value.\n",
        "      \"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"\n",
        "        Generates target sequences given a source sequence using greedy decoding.\n",
        "\n",
        "        Args:\n",
        "            source (tf.Tensor): Source sequence tensor.\n",
        "            target_start_token_idx (int): Index of the start token in the target vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            Generated target sequences tensor.\n",
        "        \"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4_DCSny-Jzc"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "Note: This requires ~3.6 GB of disk space and\n",
        "takes ~5 minutes for the extraction of files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap6AJrgh-Jzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36e7728-6e42-4427-c7e9-9207219b0851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "2748572632/2748572632 [==============================] - 211s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def download_and_extract_data(download_url, extract_to, archive_format=\"tar\"):\n",
        "    \"\"\"\n",
        "    Downloads a file from a given URL and extracts it to the specified location.\n",
        "\n",
        "    Args:\n",
        "        download_url (str): The URL to download the file from.\n",
        "        extract_to (str): The path to extract the downloaded file to.\n",
        "        archive_format (str, optional): The archive format of the downloaded file. Defaults to \"tar\".\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Download and extract the file\n",
        "    keras.utils.get_file(os.path.join(os.getcwd(), extract_to), download_url, extract=True, archive_format=archive_format, cache_dir=\".\")\n",
        "\n",
        "def get_data(wavs, id_to_text, maxlen=50):\n",
        "    \"\"\" \n",
        "    Returns a mapping of audio paths and transcription texts for the given audio files.\n",
        "\n",
        "    Args:\n",
        "        wavs (list): A list of paths to audio files.\n",
        "        id_to_text (dict): A dictionary mapping audio file IDs to their transcriptions.\n",
        "        maxlen (int, optional): The maximum length of the transcription text. Defaults to 50.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries with keys \"audio\" and \"text\", where \"audio\" is a path to an audio file\n",
        "            and \"text\" is the corresponding transcription text.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for w in wavs:\n",
        "        id = w.split(\"/\")[-1].split(\".\")[0]\n",
        "        if len(id_to_text[id]) < maxlen:\n",
        "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
        "    return data\n",
        "\n",
        "def run_data_pipeline():\n",
        "    \"\"\"\n",
        "    Downloads and extracts the LJSpeech-1.1 dataset, and returns a list of audio files and their corresponding transcription texts.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries with keys \"audio\" and \"text\", where \"audio\" is a path to an audio file\n",
        "            and \"text\" is the corresponding transcription text.\n",
        "    \"\"\"\n",
        "    # Download and extract the data\n",
        "    download_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
        "    extract_to = \"./datasets/LJSpeech-1.1\"\n",
        "    download_and_extract_data(download_url, extract_to, archive_format=\"tar\")\n",
        "\n",
        "    # Load the transcription texts\n",
        "    id_to_text = {}\n",
        "    with open(os.path.join(extract_to, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            id = line.strip().split(\"|\")[0]\n",
        "            text = line.strip().split(\"|\")[2]\n",
        "            id_to_text[id] = text\n",
        "\n",
        "    # Get the audio files and their transcriptions\n",
        "    wavs = glob(\"{}/**/*.wav\".format(extract_to), recursive=True)\n",
        "    data = get_data(wavs, id_to_text, maxlen=50)\n",
        "\n",
        "    return data\n",
        "\n",
        "run_data_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQb6RqqL-Jzd"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJHUfNMc-Jze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fc02cd-1c38-47d6-c82a-6bcda7c6e5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 34\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class VectorizeChar:\n",
        "    \"\"\"\n",
        "    A class for vectorizing characters in a given text using a pre-defined vocabulary.\n",
        "    Attributes:\n",
        "      - vocab (list): A list of characters representing the vocabulary.\n",
        "      - max_len (int): The maximum length of the input text after being pre-processed.\n",
        "      - char_to_idx (dict): A dictionary mapping characters in the vocabulary to their corresponding indices.\n",
        "\n",
        "    Methods:\n",
        "      - __call__(text): Vectorizes the given text using the pre-defined vocabulary, padding it to the specified max_len.\n",
        "      - get_vocabulary(): Returns the vocabulary used for vectorization.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len=50):\n",
        "        \"\"\"\n",
        "        Initializes the VectorizeChar class.\n",
        "        \n",
        "        Args:\n",
        "        - max_len (int): The maximum length of the input text after being pre-processed.\n",
        "        \"\"\"\n",
        "        self.vocab = (\n",
        "            [\"-\", \"#\", \"<\", \">\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\" \", \".\", \",\", \"?\"]\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "        self.char_to_idx = {}\n",
        "        for i, ch in enumerate(self.vocab):\n",
        "            self.char_to_idx[ch] = i\n",
        "\n",
        "    def __call__(self, text):\n",
        "        \"\"\"\n",
        "        Vectorizes the given text using the pre-defined vocabulary and pads it to the specified max_len.\n",
        "        \n",
        "        Args:\n",
        "        - text (str): The input text to be vectorized.\n",
        "        \n",
        "        Returns:\n",
        "        - A list of integers representing the vectorized text with padding.\n",
        "        \"\"\"\n",
        "        text = text.lower()\n",
        "        text = text[: self.max_len - 2]\n",
        "        text = \"<\" + text + \">\"\n",
        "        pad_len = self.max_len - len(text)\n",
        "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Returns the vocabulary used for vectorization.\n",
        "        \n",
        "        Returns:\n",
        "        - A list of characters representing the vocabulary.\n",
        "        \"\"\"\n",
        "        return self.vocab\n",
        "\n",
        "\n",
        "max_target_len = 200  # all transcripts in out data are < 200 characters\n",
        "data = get_data(wavs, id_to_text, max_target_len)\n",
        "vectorizer = VectorizeChar(max_target_len)\n",
        "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
        "\n",
        "\n",
        "def create_text_ds(data):\n",
        "    \"\"\"\n",
        "    Creates a Tensorflow dataset of vectorized text data from the given data dictionary.\n",
        "\n",
        "    Args:\n",
        "      - data (list): A list of dictionaries containing the \"text\" key with text data.\n",
        "\n",
        "    Returns:\n",
        "      - A Tensorflow dataset of vectorized text data.\n",
        "    \"\"\"\n",
        "    texts = [_[\"text\"] for _ in data]\n",
        "    text_ds = [vectorizer(t) for t in texts]\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
        "    return text_ds\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"\n",
        "    Converts an audio file from the given path to a spectrogram tensor using short-time Fourier transform (STFT).\n",
        "    Args:\n",
        "      - path (str): The path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "      - A Tensorflow tensor representing the spectrogram of the audio file.\n",
        "    \"\"\"\n",
        "    # spectrogram using stft\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
        "    # normalisation\n",
        "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
        "    x = (x - means) / stddevs\n",
        "    audio_len = tf.shape(x)[0]\n",
        "    # padding to 10 seconds\n",
        "    pad_len = 2754\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "def create_audio_ds(data):\n",
        "    \"\"\"\n",
        "    Creates a Tensorflow dataset of spectrogram data from the given data dictionary.\n",
        "    Args:\n",
        "      - data (list): A list of dictionaries containing the \"audio\" key with audio file paths.\n",
        "\n",
        "    Returns:\n",
        "      - A Tensorflow dataset of spectrogram data.\n",
        "    \"\"\"\n",
        "    flist = [_[\"audio\"] for _ in data]\n",
        "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
        "    audio_ds = audio_ds.map(\n",
        "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    return audio_ds\n",
        "\n",
        "\n",
        "def create_tf_dataset(data, bs=4):\n",
        "    \"\"\"\n",
        "    Creates a Tensorflow dataset from the given data dictionary with batch size bs.\n",
        "    Args:\n",
        "    - data (list): A list of dictionaries containing the \"audio\" and \"text\" keys with audio file paths and text data respectively.\n",
        "    - bs (int): The batch size for the Tensorflow dataset (default=4).\n",
        "\n",
        "    Returns:\n",
        "    - A Tensorflow dataset of audio and text pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    audio_ds = create_audio_ds(data)\n",
        "    text_ds = create_text_ds(data)\n",
        "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "    ds = ds.batch(bs)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "split = int(len(data) * 0.99)\n",
        "train_data = data[:split]\n",
        "test_data = data[split:]\n",
        "ds = create_tf_dataset(train_data, bs=64)\n",
        "val_ds = create_tf_dataset(test_data, bs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrfKA7rQ-Jzf"
      },
      "source": [
        "## Callbacks to display predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OpV1PZt-Jzf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
        "    ):\n",
        "        \"\"\"Displays a batch of outputs after every epoch\n",
        "\n",
        "        Args:\n",
        "            batch: A test batch containing the keys \"source\" and \"target\"\n",
        "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
        "            target_start_token_idx: A start token index in the target vocabulary\n",
        "            target_end_token_idx: An end token index in the target vocabulary\n",
        "        \"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 5 != 0:\n",
        "            return\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        for i in range(bs):\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
        "            prediction = \"\"\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "            print(\"\\n\")\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrMG-Snf-Jzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9548302c-d259-403c-afab-f9be3e584a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.3453\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <the as and athe the athe the the athe the the the the are the athe athe the the athe the the are the the are are are the the the are the the the the the the the the the tent tennedy.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <the as the athe the athe the the the athe the the the are the as as the athe the the the the are the the the are are the the the are the the the.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <the as the athe the athe the the as are the athe are the athe the the the the athe the the the are the the the the the the the the the the the the the the ale.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the as and athe the athe the the the athe the the the are the athe as the the athe the the the are are the an the the as the the are the the the the the the the the the tent tennedy.>\n",
            "\n",
            "203/203 [==============================] - 240s 995ms/step - loss: 1.3453 - val_loss: 1.3817\n",
            "Epoch 2/50\n",
            "203/203 [==============================] - 196s 965ms/step - loss: 1.3043 - val_loss: 1.3723\n",
            "Epoch 3/50\n",
            "203/203 [==============================] - 194s 953ms/step - loss: 1.2933 - val_loss: 1.3570\n",
            "Epoch 4/50\n",
            "203/203 [==============================] - 196s 964ms/step - loss: 1.2599 - val_loss: 1.2944\n",
            "Epoch 5/50\n",
            "203/203 [==============================] - 195s 961ms/step - loss: 1.1770 - val_loss: 1.2156\n",
            "Epoch 6/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.1202\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <the commission of the the sarried the and the sand the sand the the sarding the the prison the presiden the the the the the the the the thend the thend s te thenthendententhe the ty.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <the secret the secret the secret the secret the secret the sevent the sight.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <the commission of the the sarding the and the sand the sand the the sight the sand the sand.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the secret the secret the servent the secret the sand the sand the sight the sand the secret the secret the the the the the the the thenthend thentend tente.>\n",
            "\n",
            "203/203 [==============================] - 200s 986ms/step - loss: 1.1202 - val_loss: 1.1715\n",
            "Epoch 7/50\n",
            "203/203 [==============================] - 194s 954ms/step - loss: 1.0885 - val_loss: 1.1470\n",
            "Epoch 8/50\n",
            "203/203 [==============================] - 195s 957ms/step - loss: 1.0682 - val_loss: 1.1304\n",
            "Epoch 9/50\n",
            "203/203 [==============================] - 196s 963ms/step - loss: 1.0528 - val_loss: 1.1141\n",
            "Epoch 10/50\n",
            "203/203 [==============================] - 195s 958ms/step - loss: 1.0392 - val_loss: 1.1023\n",
            "Epoch 11/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.0288\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <the provided the provided the president and the protection of the president of the provided the president of the the the the the the the the the the the the the of pre toren kennedy.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <the secret the secretion of the secret the president and the secret the president and the president.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <the provided the arried the first were the prover the provides.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the prisoners of the president and the secret the president and the president and the secret the president anthe the thes.>\n",
            "\n",
            "203/203 [==============================] - 200s 985ms/step - loss: 1.0288 - val_loss: 1.0920\n",
            "Epoch 12/50\n",
            "203/203 [==============================] - 195s 959ms/step - loss: 1.0199 - val_loss: 1.0833\n",
            "Epoch 13/50\n",
            "203/203 [==============================] - 196s 963ms/step - loss: 1.0126 - val_loss: 1.0763\n",
            "Epoch 14/50\n",
            "203/203 [==============================] - 194s 955ms/step - loss: 1.0065 - val_loss: 1.0711\n",
            "Epoch 15/50\n",
            "203/203 [==============================] - 195s 957ms/step - loss: 1.0009 - val_loss: 1.0651\n",
            "Epoch 16/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.9951\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <the should be should be the shots all and a she shots a shots a shots a shots and a she shots and a she shot and the the the the t the there the the he anerere the prerered>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <the secret service of the service and the secret service of the secret service.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <and the was a senter the fired the first of the seem of the second the seconds.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the secret service the secret service the secret service of the secret service of the secret service of the sise the thereseseserententhenesenentes.>\n",
            "\n",
            "203/203 [==============================] - 201s 987ms/step - loss: 0.9951 - val_loss: 1.0591\n",
            "Epoch 17/50\n",
            "203/203 [==============================] - 195s 961ms/step - loss: 0.9889 - val_loss: 1.0549\n",
            "Epoch 18/50\n",
            "203/203 [==============================] - 193s 949ms/step - loss: 0.9838 - val_loss: 1.0495\n",
            "Epoch 19/50\n",
            "203/203 [==============================] - 195s 961ms/step - loss: 0.9775 - val_loss: 1.0450\n",
            "Epoch 20/50\n",
            "203/203 [==============================] - 198s 972ms/step - loss: 0.9672 - val_loss: 1.0287\n",
            "Epoch 21/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.9539\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <but the stook the stock the storth the still and the still and the stort a sto the stood the stock and the stoore the the the the the the the the the the e one the ther.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <which he was a sentence of the second the second the second to the second.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <the saw found the refore the was for the courts.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <is a station of the station of the step of the station of the station of the station of the station of the stoof ines s.>\n",
            "\n",
            "203/203 [==============================] - 202s 995ms/step - loss: 0.9539 - val_loss: 1.0161\n",
            "Epoch 22/50\n",
            "203/203 [==============================] - 195s 957ms/step - loss: 0.9394 - val_loss: 1.0028\n",
            "Epoch 23/50\n",
            "203/203 [==============================] - 198s 972ms/step - loss: 0.9187 - val_loss: 0.9753\n",
            "Epoch 24/50\n",
            "203/203 [==============================] - 196s 965ms/step - loss: 0.8914 - val_loss: 0.9888\n",
            "Epoch 25/50\n",
            "203/203 [==============================] - 196s 963ms/step - loss: 0.8744 - val_loss: 0.9268\n",
            "Epoch 26/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.8494\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <barged and and and and and and and and and and the time the time time from the time from the time from the time o an o ane on the there ofre there ther.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <was as as as relient accessary the sever of the sever of the sever of was as and>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <who had been who had been his fibervoides five been his fiberves.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the scapition the stood the scapined the seemble hopelled to the seembilities of the seemed to the seemed to the of of anthenthe of of ales,>\n",
            "\n",
            "203/203 [==============================] - 200s 979ms/step - loss: 0.8494 - val_loss: 0.9127\n",
            "Epoch 27/50\n",
            "203/203 [==============================] - 194s 952ms/step - loss: 0.8374 - val_loss: 0.8881\n",
            "Epoch 28/50\n",
            "203/203 [==============================] - 194s 953ms/step - loss: 0.8019 - val_loss: 0.8640\n",
            "Epoch 29/50\n",
            "203/203 [==============================] - 195s 958ms/step - loss: 0.7801 - val_loss: 0.8558\n",
            "Epoch 30/50\n",
            "203/203 [==============================] - 193s 951ms/step - loss: 0.7742 - val_loss: 0.8420\n",
            "Epoch 31/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.7531\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <bring the time minuted that alloway front the time minutes of the still as the still as of the shots of the stire s the s the the s s therererere be s therer.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <was and the severally as and by severally as and by severally as and by severally as and>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <the five the five block of the reflops five bed of north of the revally house.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the safer the safer the safer the seemed to the seemed to the seemed to the seemed to the secution the secupline of the one the the thes,>\n",
            "\n",
            "203/203 [==============================] - 198s 977ms/step - loss: 0.7531 - val_loss: 0.8304\n",
            "Epoch 32/50\n",
            "203/203 [==============================] - 194s 954ms/step - loss: 0.7418 - val_loss: 0.8128\n",
            "Epoch 33/50\n",
            "203/203 [==============================] - 196s 962ms/step - loss: 0.7198 - val_loss: 0.7973\n",
            "Epoch 34/50\n",
            "203/203 [==============================] - 196s 964ms/step - loss: 0.7097 - val_loss: 0.8046\n",
            "Epoch 35/50\n",
            "203/203 [==============================] - 194s 952ms/step - loss: 0.6969 - val_loss: 0.7792\n",
            "Epoch 36/50\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.6793\n",
            "\n",
            "target:     <barnett estimated that approximately three minutes elapsed between the time he heard the last of the shots and the time he started guarding the front door.>\n",
            "prediction: <barnative the time time minuted the time time time time ast of the time time time minuted the time brean earnatile the the the the the the the the ther.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <was introduced as early as seventeen ninety by mr. blackburn>\n",
            "prediction: <was inteen inteeen nine nine nine nine nine nine nineteen nineteen nineteen nine.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the five hundred block of north beckley is five blocks south of the roominghouse.>\n",
            "prediction: <the five hundred block of the block of the backley house five the block of the block of the block of the roof the.>\n",
            "\n",
            "\n",
            "\n",
            "target:     <the scaffold hung with black# and the inhabitants of the neighborhood, having petitioned the sheriffs to remove the scene of execution to the old place,>\n",
            "prediction: <the scafful home with the share hold had the share hold had the scafful hold had the sherifle hold had the seen on ble blin s s s t se s thes thenes>\n",
            "\n",
            "203/203 [==============================] - 199s 980ms/step - loss: 0.6793 - val_loss: 0.7673\n",
            "Epoch 37/50\n",
            "203/203 [==============================] - 195s 959ms/step - loss: 0.6888 - val_loss: 0.7830\n",
            "Epoch 38/50\n",
            "203/203 [==============================] - 194s 953ms/step - loss: 0.6885 - val_loss: 0.7784\n",
            "Epoch 39/50\n",
            "203/203 [==============================] - 193s 947ms/step - loss: 0.6669 - val_loss: 0.7721\n",
            "Epoch 40/50\n",
            "203/203 [==============================] - 195s 960ms/step - loss: 0.6535 - val_loss: 0.7281\n",
            "Epoch 41/50\n",
            "168/203 [=======================>......] - ETA: 33s - loss: 0.6294"
          ]
        }
      ],
      "source": [
        "batch = next(iter(val_ds))\n",
        "\n",
        "# The vocabulary to convert predicted indices into characters\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
        ")  # set the arguments as per vocabulary index for '<' and '>'\n",
        "\n",
        "model = Transformer(\n",
        "    num_hid=200,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=max_target_len,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=34,\n",
        ")\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "learning_rate = 0.0005\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs= 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "\n",
        "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n",
        "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
        "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
      ],
      "metadata": {
        "id": "VLQaMIAv5TrP"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}